{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a6d1697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First X_train data sample: \n",
      " the as you with out themselves powerful and and their becomes and had and of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every and and movie except her was several of enough more with is now and film as you of and and unfortunately of you than him that with out themselves her get for was and of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of and and with heart had and they of here that with her serious to have does when from why what have and they is you that isn't one will very to as itself with other and in of seen over and for anyone of and br and to whether from than out themselves history he name half some br of and and was two most of mean for 1 any an and she he should is thought and but of script you not while history he heart to real at and but when from one bit then have two of script their with her and most that with wasn't to with and acting watch an for with and film want an\n",
      "\n",
      " First train data sample token index sequence: \n",
      " [1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "\n",
      "Length of first train data sample token index sequence:  218\n",
      "\n",
      "First y_train data:  1\n"
     ]
    }
   ],
   "source": [
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# 난수 고정\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# data load를 위함 함수 정의\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "n_of_training_ex = 5000\n",
    "n_of_testing_ex = 1000\n",
    "\n",
    "PATH = \"data\\\\\"\n",
    "\n",
    "def imdb_data_load():\n",
    "    # data load\n",
    "    X_train = np.load(PATH + \"data_X_train.npy\")[:n_of_training_ex]\n",
    "    y_train = np.load(PATH + \"data_y_train.npy\")[:n_of_training_ex]\n",
    "    X_test = np.load(PATH + \"data_X_test.npy\")[:n_of_testing_ex]\n",
    "    y_test = np.load(PATH + \"data_y_test.npy\")[:n_of_testing_ex]\n",
    "    \n",
    "    # json 파일에 저장된 단어 index 불러오기\n",
    "    with open(PATH + \"data_imdb_word_index.json\") as f:\n",
    "        word_index = json.load(f)\n",
    "    # Dictionary의 \"단어: Index\" 를 \"Index: 단어\" 로 변환\n",
    "    inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
    "    # 인덱스를 기준 단어를 문장으로 변환\n",
    "    decoded_sequence = \" \".join(inverted_word_index[i] for i in X_train[0])\n",
    "    \n",
    "    print(\"First X_train data sample: \\n\", decoded_sequence)\n",
    "    print(\"\\n First train data sample token index sequence: \\n\", X_train[0])\n",
    "    print(\"\\nLength of first train data sample token index sequence: \", len(X_train[0]))\n",
    "    print(\"\\nFirst y_train data: \", y_train[0])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = imdb_data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee132755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Padding> First X_train data sample token index sequence: \n",
      " [  1  14  22  16  43 530 973   2   2  65 458   2  66   2   4 173  36 256\n",
      "   5  25 100  43 838 112  50 670   2   9  35 480 284   5 150   4 172 112\n",
      " 167   2 336 385  39   4 172   2   2  17 546  38  13 447   4 192  50  16\n",
      "   6 147   2  19  14  22   4   2   2 469   4  22  71  87  12  16  43 530\n",
      "  38  76  15  13   2   4  22  17 515  17  12  16 626  18   2   5  62 386\n",
      "  12   8 316   8 106   5   4   2   2  16 480  66   2  33   4 130  12  16\n",
      "  38 619   5  25 124  51  36 135  48  25   2  33   6  22  12 215  28  77\n",
      "  52   5  14 407  16  82   2   8   4 107 117   2  15 256   4   2   7   2\n",
      "   5 723  36  71  43 530 476  26 400 317  46   7   4   2   2  13 104  88\n",
      "   4 381  15 297  98  32   2  56  26 141   6 194   2  18   4 226  22  21\n",
      " 134 476  26 480   5 144  30   2  18  51  36  28 224  92  25 104   4 226\n",
      "  65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16   2  19\n",
      " 178  32   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# padding 수행\n",
    "# maxlen: 최대 길이 설정 (넘으면 자른다)\n",
    "# padding: default=pre (왼쪽에 0) / padding=-1 (-1) / padding='post' (오른쪽 0)\n",
    "\n",
    "max_review_length = 300\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post')\n",
    "\n",
    "print(\"\\n<Padding> First X_train data sample token index sequence: \\n\", X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36d329b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 32)           32000     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 5)                 190       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 32,196\n",
      "Trainable params: 32,196\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "157/157 - 5s - loss: 0.6925 - accuracy: 0.5248 - 5s/epoch - 32ms/step\n",
      "Epoch 2/5\n",
      "157/157 - 4s - loss: 0.6814 - accuracy: 0.5680 - 4s/epoch - 27ms/step\n",
      "Epoch 3/5\n",
      "157/157 - 4s - loss: 0.6742 - accuracy: 0.5806 - 4s/epoch - 27ms/step\n",
      "Epoch 4/5\n",
      "157/157 - 4s - loss: 0.6640 - accuracy: 0.6014 - 4s/epoch - 27ms/step\n",
      "Epoch 5/5\n",
      "157/157 - 4s - loss: 0.6515 - accuracy: 0.5998 - 4s/epoch - 27ms/step\n"
     ]
    }
   ],
   "source": [
    "# 모델 구현\n",
    "embedding_vector_length = 32\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(1000, embedding_vector_length, input_length = max_review_length),   # Word Embedding\n",
    "    tf.keras.layers.SimpleRNN(5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')   # 이진분류라서 output을 1로 설정\n",
    "])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# 모델 학습 방법 설정\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "model_history = model.fit(X_train, y_train, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9afe39cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.6805 | Test accuracy: 0.5490000247955322\n",
      "\n",
      "Predicted test data class:  0\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# 예측\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('\\nTest loss: {:.4f} | Test accuracy: {}'.format(loss, test_acc))\n",
    "print('\\nPredicted test data class: ', 1 if predictions[0]>=0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29426af7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
